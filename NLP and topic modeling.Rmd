---
title: "Text Mining on Meta Data of the New Submitted CS Paper from Archive"
author: "Cherlane"
date: "June 30, 2019"
output: html_document
---

Function "ipak" is a powerful function developed by Yifan Wu, a PhD student from department of statistics and actuarial science. To load all the required packages/library in our analysis, we suggest to run this function at the beginning.
```{r,warning=FALSE,message=FALSE}
# Function Ipak for Detecting and Installing the Packages
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}


# The list/vector with the name of the packages you need in the further analysis coding.
packages <- c("tidyverse",  # General-purpose data wrangling
              "stringr",    # String manipulation
              "rebus",      # Verbose regular expressions
              "lubridate",   # Eases DateTime manipulation
              "RPostgreSQL", # Package for communicating to PostgreSQL
              "topicmodels", # Topic Modelling
              "tidytext",   # Tidy the Text
              "ggplot2",    # Elegent Plotting
              "wordcloud","RColorBrewer",  #Wordcloud
              "tm")    # Text Mining for Topic Modeling Preparation

# Passing the list/vector of package name to the function to check the installation of the packages
ipak(packages)
```


### Import a dataset containing all the meta data for the paper submitted from June 27 to Today, July 2.
```{r}
pg <- dbDriver("PostgreSQL")
con <- dbConnect(pg, host="localhost", user="postgres",password = "aptx0330")
df.existed = dbGetQuery(con, "SELECT * from cs_paper_meta")
dbDisconnect(con)
```

## Text Mining on Title of Papers
```{r}
title.exist = df.existed$title
title_df = tibble(line = 1:length(title.exist), text = title.exist)%>%
  unnest_tokens(word,text)%>% # Broken the titles into words
  anti_join(stop_words)       # Remove the stop words
head(title_df, n = 30)
```

### Some summary after the text cleaning
```{r}
title_word_summary = title_df %>%
  count(word, sort = TRUE) 
head(title_word_summary, n = 20)

title_df %>%
  count(word, sort = TRUE) %>%
  filter(n > 25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

### Wordcloud for the tile words
```{r}
wordcloud(title_word_summary$word,title_word_summary$n,min.freq=10,colors=brewer.pal(8, "Dark2"))
```

### Topic Modeling of the Description Using Latent Dirichlet allocation (LDA)
```{r}
#Clean Text
title.exist = gsub("[[:punct:]]","", title.exist)
title.exist = gsub("[[:digit:]]", "", title.exist)
title.exist = gsub("http\\w+", "", title.exist)
title.exist = gsub("[ \t]{2,}", "", title.exist)
title.exist = gsub("^\\s+|\\s+$", "", title.exist)

title.corpus = Corpus(VectorSource(title.exist))
title.corpus = tm_map(title.corpus,removePunctuation)
title.corpus = tm_map(title.corpus,stripWhitespace)
title.corpus = tm_map(title.corpus,tolower)
title.corpus = tm_map(title.corpus,removeWords,stopwords("english"))
title.corpus = tm_map(title.corpus,removeWords,c("can", "model", "using","method","approach","proposed","propose","use","show"))
title.tdm = DocumentTermMatrix(title.corpus) # Creating a Term document Matrix
```

### Building the topic model assuming there are 5 topic
```{r}
title_lda <- LDA(title.tdm, k = 5, control = list(seed = 1234))
title_lda
```

### Showing the top 5 words of the 
```{r}
title_topics <- tidy(title_lda, matrix = "beta")

top_terms <- title_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

#### Based on the wordcloud and the topic models on paper titles, we can see that deep learning with neural network is the hottest topic recently. Moreover, the study of the adversarial problem of the graphs or image deep learning is really popular

## Text Mining on Description of the Papers
```{r}
des.exist = df.existed$description
des_df = tibble(line = 1:length(des.exist), text = des.exist)%>%
  unnest_tokens(word,text)%>% # Broken the titles into words
  anti_join(stop_words)       # Remove the stop words
head(des_df, n = 30)
```

### Some summary after the text cleaning
```{r}
des_word_summary = des_df %>%
  count(word, sort = TRUE) 
head(des_word_summary, n = 20)

des_df %>%
  count(word, sort = TRUE) %>%
  filter(n > 150) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


### Wordcloud for the description words
```{r}
wordcloud(des_word_summary$word,des_word_summary$n,min.freq=100,colors=brewer.pal(8, "Dark2"))
```


### Topic Modeling of the Description Using Latent Dirichlet allocation (LDA)
```{r}
#Clean Text
des.exist = gsub("[[:punct:]]","", des.exist)
des.exist = gsub("[[:digit:]]", "", des.exist)
des.exist = gsub("http\\w+", "", des.exist)
des.exist = gsub("[ \t]{2,}", "", des.exist)
des.exist = gsub("^\\s+|\\s+$", "", des.exist)

des.corpus = Corpus(VectorSource(des.exist))
des.corpus = tm_map(des.corpus,removePunctuation)
des.corpus = tm_map(des.corpus,stripWhitespace)
des.corpus = tm_map(des.corpus,tolower)
des.corpus = tm_map(des.corpus,removeWords,stopwords("english"))
des.corpus = tm_map(des.corpus,removeWords,c("data", "can", "model", "using","paper","papers","method","approach","proposed","propose","use","show"))
des.tdm = DocumentTermMatrix(des.corpus) # Creating a Term document Matrix
```

### Building the topic model assuming there are 5 topic
```{r}
des_lda <- LDA(des.tdm, k = 5, control = list(seed = 1234))
des_lda
```

### Showing the top 5 words of the 
```{r}
des_topics <- tidy(des_lda, matrix = "beta")

top_terms <- des_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

 



