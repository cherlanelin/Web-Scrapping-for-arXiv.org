---
title: "sample for web scraping"
author: "Cherlane"
date: "June 27, 2019"
output: html_document
---

Function "ipak" is a powerful function developed by Yifan Wu, a PhD student from department of statistics and actuarial science. To load all the required packages/library in our analysis, we suggest to run this function at the beginning.
```{r,warning=FALSE,message=FALSE}
# Function Ipak for Detecting and Installing the Packages
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}


# The list/vector with the name of the packages you need in the further analysis coding.
packages <- c("tidyverse",  # General-purpose data wrangling
              "rvest",  # Parsing of HTML/XML files  
              "stringr",    # String manipulation
              "rebus",      # Verbose regular expressions
              "lubridate",   # Eases DateTime manipulation
              "aRxiv",   # Package for communicating to aRvix.org API and get update information
              "RPostgreSQL", # Package for communicating to PostgreSQL
              "RCurl",   
              "rjson")

# Passing the list/vector of package name to the function to check the installation of the packages
ipak(packages)
```

### Check the number of new papers
```{r}
url.full = "https://arxiv.org/list/cs/new"
webpage.full = read_html(url.full)
try = webpage.full%>%html_nodes("#dlpage > dl:nth-child(10)")%>%html_text
try.2= unlist(strsplit(try,split='\n\n\n\n[', fixed=TRUE))
length(try.2)
```


### Read the HTML code from the website with latest 20 submissions
```{r}
n = 40
url = paste("https://arxiv.org/list/cs/new?skip=0&show=",n,sep="")


#Reading the HTML code from the website
webpage <- read_html(url)

webpage%>%html_attrs()

## Get the Link of the newest paper
link.1=webpage%>%html_nodes("#dlpage > dl:nth-child(10) > dt:nth-child(1) > span:nth-child(2) > a:nth-child(1)")%>%html_text

## Get the Title of the newest paper
all.1 = webpage%>%html_nodes("#dlpage > dl:nth-child(10) > dd:nth-child(2) > div:nth-child(1) > div:nth-child(1)")%>%html_text

## Get All the information of the newest paper
all.1 = webpage%>%html_nodes("#dlpage > dl:nth-child(10) > dd:nth-child(2) > div:nth-child(1)")%>%html_text

## Get All the information of the second newest paper
all.2 = webpage%>%html_nodes("#dlpage > dl:nth-child(10) > dd:nth-child(4) > div:nth-child(1)")%>%html_text
```

### Extract the title, author name and description with some text cleaning
```{r}
all.1.clean = unlist(strsplit(all.1,split='\n\n\n', fixed=TRUE))
all.1.clean

all.1.title = substr(all.1.clean[1],10,nchar(all.1.clean[1]))
all.1.title

all.1.author= str_remove_all(substr(all.1.clean[2],10,nchar(all.1.clean[2])), "\n")
all.1.author

all.1.description = str_remove_all(all.1.clean[length(all.1.clean)],"\n")
all.1.description
```


### Clean the Link and Make-up the Possible HTML Address
* The http address link looks like: https://arxiv.org/abs/1906.11282
* The link we have looks like: arXiv:1906.11282
```{r}
link.1.make = paste("https://arxiv.org/abs/", substr(link.1,7,nchar(link.1)),sep = "")
link.1.make
```

### Function for Getting the information of the newest n-th papers from the total N newest paper 
```{r}
N = 40
url = paste("https://arxiv.org/list/cs/new?skip=0&show=",N,sep="")
#Reading the HTML code from the website
webpage <- read_html(url)

get.nth.paper = function(n,N,webpage){
  if (n>N){
    print("Exceeding the limit N, only give the information of the N-th newest paper ")
    n = N}
  
    webpage.local = webpage
    ## Get the Link of the newest paper
    link=webpage.local%>%html_nodes(paste("#dlpage > dl:nth-child(10) > dt:nth-child(",2*n-1,") > span:nth-child(2) > a:nth-child(1)", sep = ""))%>%html_text
    
    link.make = paste("https://arxiv.org/abs/", substr(link,7,nchar(link)),sep = "")
    
    ## Get All the information of the newest paper
    all= webpage.local%>%html_nodes(paste("#dlpage > dl:nth-child(10) > dd:nth-child(",2*n,") > div:nth-child(1)", sep = ""))%>%html_text
    
    all.clean = unlist(strsplit(all,split='\n\n\n', fixed=TRUE))
  
    all.title = substr(all.clean[1],10,nchar(all.clean[1]))
    
    all.author= str_remove_all(substr(all.clean[2],10,nchar(all.clean[2])), "\n")
    
    all.description = str_remove_all(all.clean[length(all.clean)],"\n")
  
    all.final = data.frame(link = link,
                           link_makeup = link.make,
                           title = all.title,
                           author = all.author,
                           description = all.description)
    return(all.final)
}

## Test
get.nth.paper(35,40,webpage)
```

### Function for getting the information of the first N paper from https://arxiv.org/list/cs/new
```{r}
get.first.N.paper = function(N){
  all.N = data.frame(matrix(ncol = 5, nrow = 1))
  colnames(all.N) <- c("link", "link_makeup", 
                       "title", "author", 
                       "description")
  url.local = paste("https://arxiv.org/list/cs/new?skip=0&show=",N,sep="")
  #Reading the HTML code from the website
  webpage.local <- read_html(url.local)
  for (n in 1:N){
     nth.paper = get.nth.paper(n,N,webpage.local)
     all.N = rbind(all.N, nth.paper)
  }
all.N = all.N[-1,]
return(all.N)
}

## Test
first.70 = get.first.N.paper(70)
```


### Function for checking the number of new submission papers on the website
```{r}
get.new.num = function(){
   url.full = "https://arxiv.org/list/cs/new"
   webpage.full = read_html(url.full)
   all.papers= webpage.full%>%html_nodes("#dlpage > dl:nth-child(10)")%>%html_text
   all.papers.num = length(unlist(strsplit(all.papers,split='\n\n\n\n[', fixed=TRUE)))
   return(all.papers.num)
}

# test
get.new.num()
```


### Function for getting the information of the latest N paper from https://arxiv.org/list/cs/new
```{r}
get.last.N.paper = function(N){
  all.N = data.frame(matrix(ncol = 5, nrow = 1))
  colnames(all.N) <- c("link", "link_makeup", 
                       "title", "author", 
                       "description")
  n.new = get.new.num()
  url.full = "https://arxiv.org/list/cs/new"
  webpage.full = read_html(url.full)
  for (n in n.new:(n.new-N+1)){
     nth.paper = get.nth.paper(n,n.new,webpage.full)
     all.N = rbind(all.N, nth.paper)
  }
  all.N = all.N[-1,]
  return(all.N)
}

latest.70 = get.last.N.paper(70)
```


### Function for getting all new submission papers on the website
```{r}
get.all.paper = function(){
  n.all = get.new.num()
  df.all = get.first.N.paper(n.all)
  return(df.all)
}

# test 
all.new.paper = get.all.paper()
```


### Connecting to PostgreSQL and create the initial dataset 'cs_paper_meta'
```{r}
pg <- dbDriver("PostgreSQL")
con <- dbConnect(pg, host="localhost", user="postgres",password = "aptx0330")

# # Create a table in postgre SQL with the meta data collect from June 27 to June 29, 2019
# n.new = arxiv_count('(cat:cs.AI OR cat:cs.CL OR cat:cs.CC OR cat:cs.CE OR cat:cs.CG OR cat:cs.GT OR cat:cs.CV OR cat:cs.CY OR cat:cs.CR OR cat:cs.DS OR cat:cs.DB OR cat:cs.DL OR cat:cs.DM OR cat:cs.DC OR cat:cs.ET OR cat:cs.FL OR cat:cs.GL OR cat:cs.GR OR cat:cs.AR OR cat:cs.HC OR cat:cs.IR OR cat:cs.IT OR cat:cs.LO OR cat:cs.LG OR cat:cs.MS OR cat:cs.MA OR cat:cs.MM OR cat:cs.NI OR cat:cs.NE OR cat:cs.NA OR cat:cs.OS OR cat:cs.OH OR cat:cs.PF OR cat:cs.PL OR cat:cs.RO OR cat:cs.SI OR cat:cs.SE OR cat:cs.SD OR cat:cs.SC OR cat:cs.SY OR cat:cs.CC) AND submittedDate:[201906270000* TO 201906290000*]')
# n.new
# df = get.all.N.paper(n.new)
# 
# # Reverse the order to make sure the latest paper is in the last line
# df = df%>%map_df(rev)
# dbWriteTable(con,'cs_paper_meta',all.new.paper, row.names=FALSE)
```


### Function for checking the new submission in all new submissions and updating the existed dataframe
```{r}
get.new.submission = function(new.paper){
  pg <- dbDriver("PostgreSQL")
  con <- dbConnect(pg, host="localhost", user="postgres",password = "aptx0330")
  df.existed.link = dbGetQuery(con, "SELECT link from cs_paper_meta")$link
  paper.new.submit = subset(new.paper, !(new.paper$link %in% df.existed.link))
  if ((nrow(na.omit(paper.new.submit))) != 0){
  dbWriteTable(con,'cs_paper_meta',paper.new.submit, append = TRUE, row.names=FALSE)
  }else{
  print("All the papers from the scrapping are already existed in our database")
}

dbDisconnect(con)  
}

get.new.submission(paper.new.submit)

```



